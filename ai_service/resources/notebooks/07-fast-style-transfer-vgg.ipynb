{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "\"\"\"    \n",
    "    https://github.com/pytorch/vision/blob/3c254fb7af5f8af252c24e89949c54a3461ff0be/torchvision/models/vgg.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Vgg16(torch.nn.Module):\n",
    "    \"\"\"Only those layers are exposed which have already proven to work nicely.\"\"\"\n",
    "    def __init__(self, requires_grad=False, show_progress=False):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=True, progress=show_progress).features\n",
    "        self.layer_names = ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3']\n",
    "        self.content_feature_maps_index = 1  # relu2_2\n",
    "        self.style_feature_maps_indices = list(range(len(self.layer_names)))  # all layers used for style representation\n",
    "\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.slice1(x)\n",
    "        relu1_2 = x\n",
    "        x = self.slice2(x)\n",
    "        relu2_2 = x\n",
    "        x = self.slice3(x)\n",
    "        relu3_3 = x\n",
    "        x = self.slice4(x)\n",
    "        relu4_3 = x\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n",
    "        out = vgg_outputs(relu1_2, relu2_2, relu3_3, relu4_3)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Vgg16Experimental(torch.nn.Module):\n",
    "    \"\"\"Everything exposed so you can play with different combinations for style and content representation\"\"\"\n",
    "    def __init__(self, requires_grad=False, show_progress=False):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=True, progress=show_progress).features\n",
    "        self.layer_names = ['relu1_1', 'relu2_1', 'relu2_2', 'relu3_1', 'relu3_2', 'relu4_1', 'relu4_3', 'relu5_1']\n",
    "        self.content_feature_maps_index = 4\n",
    "        self.style_feature_maps_indices = list(range(len(self.layer_names)))  # all layers used for style representation\n",
    "\n",
    "        self.conv1_1 = vgg_pretrained_features[0]\n",
    "        self.relu1_1 = vgg_pretrained_features[1]\n",
    "        self.conv1_2 = vgg_pretrained_features[2]\n",
    "        self.relu1_2 = vgg_pretrained_features[3]\n",
    "        self.max_pooling1 = vgg_pretrained_features[4]\n",
    "        self.conv2_1 = vgg_pretrained_features[5]\n",
    "        self.relu2_1 = vgg_pretrained_features[6]\n",
    "        self.conv2_2 = vgg_pretrained_features[7]\n",
    "        self.relu2_2 = vgg_pretrained_features[8]\n",
    "        self.max_pooling2 = vgg_pretrained_features[9]\n",
    "        self.conv3_1 = vgg_pretrained_features[10]\n",
    "        self.relu3_1 = vgg_pretrained_features[11]\n",
    "        self.conv3_2 = vgg_pretrained_features[12]\n",
    "        self.relu3_2 = vgg_pretrained_features[13]\n",
    "        self.conv3_3 = vgg_pretrained_features[14]\n",
    "        self.relu3_3 = vgg_pretrained_features[15]\n",
    "        self.max_pooling3 = vgg_pretrained_features[16]\n",
    "        self.conv4_1 = vgg_pretrained_features[17]\n",
    "        self.relu4_1 = vgg_pretrained_features[18]\n",
    "        self.conv4_2 = vgg_pretrained_features[19]\n",
    "        self.relu4_2 = vgg_pretrained_features[20]\n",
    "        self.conv4_3 = vgg_pretrained_features[21]\n",
    "        self.relu4_3 = vgg_pretrained_features[22]\n",
    "        self.max_pooling4 = vgg_pretrained_features[23]\n",
    "        self.conv5_1 = vgg_pretrained_features[24]\n",
    "        self.relu5_1 = vgg_pretrained_features[25]\n",
    "        self.conv5_2 = vgg_pretrained_features[26]\n",
    "        self.relu5_2 = vgg_pretrained_features[27]\n",
    "        self.conv5_3 = vgg_pretrained_features[28]\n",
    "        self.relu5_3 = vgg_pretrained_features[29]\n",
    "        self.max_pooling5 = vgg_pretrained_features[30]\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_1(x)\n",
    "        conv1_1 = x\n",
    "        x = self.relu1_1(x)\n",
    "        relu1_1 = x\n",
    "        x = self.conv1_2(x)\n",
    "        conv1_2 = x\n",
    "        x = self.relu1_2(x)\n",
    "        relu1_2 = x\n",
    "        x = self.max_pooling1(x)\n",
    "        x = self.conv2_1(x)\n",
    "        conv2_1 = x\n",
    "        x = self.relu2_1(x)\n",
    "        relu2_1 = x\n",
    "        x = self.conv2_2(x)\n",
    "        conv2_2 = x\n",
    "        x = self.relu2_2(x)\n",
    "        relu2_2 = x\n",
    "        x = self.max_pooling2(x)\n",
    "        x = self.conv3_1(x)\n",
    "        conv3_1 = x\n",
    "        x = self.relu3_1(x)\n",
    "        relu3_1 = x\n",
    "        x = self.conv3_2(x)\n",
    "        conv3_2 = x\n",
    "        x = self.relu3_2(x)\n",
    "        relu3_2 = x\n",
    "        x = self.conv3_3(x)\n",
    "        conv3_3 = x\n",
    "        x = self.relu3_3(x)\n",
    "        relu3_3 = x\n",
    "        x = self.max_pooling3(x)\n",
    "        x = self.conv4_1(x)\n",
    "        conv4_1 = x\n",
    "        x = self.relu4_1(x)\n",
    "        relu4_1 = x\n",
    "        x = self.conv4_2(x)\n",
    "        conv4_2 = x\n",
    "        x = self.relu4_2(x)\n",
    "        relu4_2 = x\n",
    "        x = self.conv4_3(x)\n",
    "        conv4_3 = x\n",
    "        x = self.relu4_3(x)\n",
    "        relu4_3 = x\n",
    "        x = self.max_pooling4(x)\n",
    "        x = self.conv5_1(x)\n",
    "        conv5_1 = x\n",
    "        x = self.relu5_1(x)\n",
    "        relu5_1 = x\n",
    "        x = self.conv5_2(x)\n",
    "        conv5_2 = x\n",
    "        x = self.relu5_2(x)\n",
    "        relu5_2 = x\n",
    "        x = self.conv5_3(x)\n",
    "        conv5_3 = x\n",
    "        x = self.relu5_3(x)\n",
    "        relu5_3 = x\n",
    "        x = self.max_pooling5(x)\n",
    "        # expose only the layers that you want to experiment with here\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n",
    "        out = vgg_outputs(relu1_1, relu2_1, relu2_2, relu3_1, relu3_2, relu4_1, relu4_3, relu5_1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Vgg19(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Used in the original NST paper, only those layers are exposed which were used in the original paper\n",
    "\n",
    "    'conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1' were used for style representation\n",
    "    'conv4_2' was used for content representation (although they did some experiments with conv2_2 and conv5_2)\n",
    "    \"\"\"\n",
    "    def __init__(self, requires_grad=False, show_progress=False, use_relu=True):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = models.vgg19(pretrained=True, progress=show_progress).features\n",
    "        if use_relu:  # use relu or as in original paper conv layers\n",
    "            self.layer_names = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'conv4_2', 'relu5_1']\n",
    "            self.offset = 1\n",
    "        else:\n",
    "            self.layer_names = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv4_2', 'conv5_1']\n",
    "            self.offset = 0\n",
    "        self.content_feature_maps_index = 4  # conv4_2\n",
    "        # all layers used for style representation except conv4_2\n",
    "        self.style_feature_maps_indices = list(range(len(self.layer_names)))\n",
    "        self.style_feature_maps_indices.remove(4)  # conv4_2\n",
    "\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.slice6 = torch.nn.Sequential()\n",
    "        for x in range(1+self.offset):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(1+self.offset, 6+self.offset):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(6+self.offset, 11+self.offset):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(11+self.offset, 20+self.offset):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(20+self.offset, 22):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(22, 29++self.offset):\n",
    "            self.slice6.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.slice1(x)\n",
    "        layer1_1 = x\n",
    "        x = self.slice2(x)\n",
    "        layer2_1 = x\n",
    "        x = self.slice3(x)\n",
    "        layer3_1 = x\n",
    "        x = self.slice4(x)\n",
    "        layer4_1 = x\n",
    "        x = self.slice5(x)\n",
    "        conv4_2 = x\n",
    "        x = self.slice6(x)\n",
    "        layer5_1 = x\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n",
    "        out = vgg_outputs(layer1_1, layer2_1, layer3_1, layer4_1, conv4_2, layer5_1)\n",
    "        return out"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import utils.utils as utils\n",
    "from utils.video_utils import create_video_from_intermediate_results\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, LBFGS\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "def build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n",
    "    target_content_representation = target_representations[0]\n",
    "    target_style_representation = target_representations[1]\n",
    "\n",
    "    current_set_of_feature_maps = neural_net(optimizing_img)\n",
    "\n",
    "    current_content_representation = current_set_of_feature_maps[content_feature_maps_index].squeeze(axis=0)\n",
    "    content_loss = torch.nn.MSELoss(reduction='mean')(target_content_representation, current_content_representation)\n",
    "\n",
    "    style_loss = 0.0\n",
    "    current_style_representation = [utils.gram_matrix(x) for cnt, x in enumerate(current_set_of_feature_maps) if cnt in style_feature_maps_indices]\n",
    "    for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):\n",
    "        style_loss += torch.nn.MSELoss(reduction='sum')(gram_gt[0], gram_hat[0])\n",
    "    style_loss /= len(target_style_representation)\n",
    "\n",
    "    tv_loss = utils.total_variation(optimizing_img)\n",
    "\n",
    "    total_loss = config['content_weight'] * content_loss + config['style_weight'] * style_loss + config['tv_weight'] * tv_loss\n",
    "\n",
    "    return total_loss, content_loss, style_loss, tv_loss\n",
    "\n",
    "\n",
    "def make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n",
    "    # Builds function that performs a step in the tuning loop\n",
    "    def tuning_step(optimizing_img):\n",
    "        total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config)\n",
    "        # Computes gradients\n",
    "        total_loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return total_loss, content_loss, style_loss, tv_loss\n",
    "\n",
    "    # Returns the function that will be called inside the tuning loop\n",
    "    return tuning_step\n",
    "\n",
    "\n",
    "def neural_style_transfer(config):\n",
    "    content_img_path = os.path.join(config['content_images_dir'], config['content_img_name'])\n",
    "    style_img_path = os.path.join(config['style_images_dir'], config['style_img_name'])\n",
    "\n",
    "    out_dir_name = 'combined_' + os.path.split(content_img_path)[1].split('.')[0] + '_' + os.path.split(style_img_path)[1].split('.')[0]\n",
    "    dump_path = os.path.join(config['output_img_dir'], out_dir_name)\n",
    "    os.makedirs(dump_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    content_img = utils.prepare_img(content_img_path, config['height'], device)\n",
    "    style_img = utils.prepare_img(style_img_path, config['height'], device)\n",
    "\n",
    "    if config['init_method'] == 'random':\n",
    "        # white_noise_img = np.random.uniform(-90., 90., content_img.shape).astype(np.float32)\n",
    "        gaussian_noise_img = np.random.normal(loc=0, scale=90., size=content_img.shape).astype(np.float32)\n",
    "        init_img = torch.from_numpy(gaussian_noise_img).float().to(device)\n",
    "    elif config['init_method'] == 'content':\n",
    "        init_img = content_img\n",
    "    else:\n",
    "        # init image has same dimension as content image - this is a hard constraint\n",
    "        # feature maps need to be of same size for content image and init image\n",
    "        style_img_resized = utils.prepare_img(style_img_path, np.asarray(content_img.shape[2:]), device)\n",
    "        init_img = style_img_resized\n",
    "\n",
    "    # we are tuning optimizing_img's pixels! (that's why requires_grad=True)\n",
    "    optimizing_img = Variable(init_img, requires_grad=True)\n",
    "\n",
    "    neural_net, content_feature_maps_index_name, style_feature_maps_indices_names = utils.prepare_model(config['model'], device)\n",
    "    print(f'Using {config[\"model\"]} in the optimization procedure.')\n",
    "\n",
    "    content_img_set_of_feature_maps = neural_net(content_img)\n",
    "    style_img_set_of_feature_maps = neural_net(style_img)\n",
    "\n",
    "    target_content_representation = content_img_set_of_feature_maps[content_feature_maps_index_name[0]].squeeze(axis=0)\n",
    "    target_style_representation = [utils.gram_matrix(x) for cnt, x in enumerate(style_img_set_of_feature_maps) if cnt in style_feature_maps_indices_names[0]]\n",
    "    target_representations = [target_content_representation, target_style_representation]\n",
    "\n",
    "    # magic numbers in general are a big no no - some things in this code are left like this by design to avoid clutter\n",
    "    num_of_iterations = {\n",
    "        \"lbfgs\": 1000,\n",
    "        \"adam\": 3000,\n",
    "    }\n",
    "\n",
    "    #\n",
    "    # Start of optimization procedure\n",
    "    #\n",
    "    if config['optimizer'] == 'adam':\n",
    "        optimizer = Adam((optimizing_img,), lr=1e1)\n",
    "        tuning_step = make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n",
    "        for cnt in range(num_of_iterations[config['optimizer']]):\n",
    "            total_loss, content_loss, style_loss, tv_loss = tuning_step(optimizing_img)\n",
    "            with torch.no_grad():\n",
    "                print(f'Adam | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n",
    "                utils.save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=False)\n",
    "    elif config['optimizer'] == 'lbfgs':\n",
    "        # line_search_fn does not seem to have significant impact on result\n",
    "        optimizer = LBFGS((optimizing_img,), max_iter=num_of_iterations['lbfgs'], line_search_fn='strong_wolfe')\n",
    "        cnt = 0\n",
    "\n",
    "        def closure():\n",
    "            nonlocal cnt\n",
    "            if torch.is_grad_enabled():\n",
    "                optimizer.zero_grad()\n",
    "            total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n",
    "            if total_loss.requires_grad:\n",
    "                total_loss.backward()\n",
    "            with torch.no_grad():\n",
    "                print(f'L-BFGS | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n",
    "                utils.save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=False)\n",
    "\n",
    "            cnt += 1\n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return dump_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #\n",
    "    # fixed args - don't change these unless you have a good reason\n",
    "    #\n",
    "    default_resource_dir = os.path.join(os.path.dirname(__file__), 'data')\n",
    "    content_images_dir = os.path.join(default_resource_dir, 'content-images')\n",
    "    style_images_dir = os.path.join(default_resource_dir, 'style-images')\n",
    "    output_img_dir = os.path.join(default_resource_dir, 'output-images')\n",
    "    img_format = (4, '.jpg')  # saves images in the format: %04d.jpg\n",
    "\n",
    "    #\n",
    "    # modifiable args - feel free to play with these (only small subset is exposed by design to avoid cluttering)\n",
    "    # sorted so that the ones on the top are more likely to be changed than the ones on the bottom\n",
    "    #\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--content_img_name\", type=str, help=\"content image name\", default='figures.jpg')\n",
    "    parser.add_argument(\"--style_img_name\", type=str, help=\"style image name\", default='vg_starry_night.jpg')\n",
    "    parser.add_argument(\"--height\", type=int, help=\"height of content and style images\", default=400)\n",
    "\n",
    "    parser.add_argument(\"--content_weight\", type=float, help=\"weight factor for content loss\", default=1e5)\n",
    "    parser.add_argument(\"--style_weight\", type=float, help=\"weight factor for style loss\", default=3e4)\n",
    "    parser.add_argument(\"--tv_weight\", type=float, help=\"weight factor for total variation loss\", default=1e0)\n",
    "\n",
    "    parser.add_argument(\"--optimizer\", type=str, choices=['lbfgs', 'adam'], default='lbfgs')\n",
    "    parser.add_argument(\"--model\", type=str, choices=['vgg16', 'vgg19'], default='vgg19')\n",
    "    parser.add_argument(\"--init_method\", type=str, choices=['random', 'content', 'style'], default='content')\n",
    "    parser.add_argument(\"--saving_freq\", type=int, help=\"saving frequency for intermediate images (-1 means only final)\", default=-1)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # some values of weights that worked for figures.jpg, vg_starry_night.jpg (starting point for finding good images)\n",
    "    # once you understand what each one does it gets really easy -> also see README.md\n",
    "\n",
    "    # lbfgs, content init -> (cw, sw, tv) = (1e5, 3e4, 1e0)\n",
    "    # lbfgs, style   init -> (cw, sw, tv) = (1e5, 1e1, 1e-1)\n",
    "    # lbfgs, random  init -> (cw, sw, tv) = (1e5, 1e3, 1e0)\n",
    "\n",
    "    # adam, content init -> (cw, sw, tv, lr) = (1e5, 1e5, 1e-1, 1e1)\n",
    "    # adam, style   init -> (cw, sw, tv, lr) = (1e5, 1e2, 1e-1, 1e1)\n",
    "    # adam, random  init -> (cw, sw, tv, lr) = (1e5, 1e2, 1e-1, 1e1)\n",
    "\n",
    "    # just wrapping settings into a dictionary\n",
    "    optimization_config = dict()\n",
    "    for arg in vars(args):\n",
    "        optimization_config[arg] = getattr(args, arg)\n",
    "    optimization_config['content_images_dir'] = content_images_dir\n",
    "    optimization_config['style_images_dir'] = style_images_dir\n",
    "    optimization_config['output_img_dir'] = output_img_dir\n",
    "    optimization_config['img_format'] = img_format\n",
    "\n",
    "    # original NST (Neural Style Transfer) algorithm (Gatys et al.)\n",
    "    results_path = neural_style_transfer(optimization_config)\n",
    "\n",
    "    # uncomment this if you want to create a video from images dumped during the optimization procedure\n",
    "    # create_video_from_intermediate_results(results_path, img_format)"
   ],
   "id": "1bd264d1cb92bd45"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
